---
draft: "true"
date: 2024-03-24T15:42:15.1515+08:00
last-modified: 2024-05-01T16:09:42.4242+08:00
---


# PPO

## reference

- https://d2jud02ci9yv69.cloudfront.net/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo-130/blog/the-n-implementation-details-of-rlhf-with-ppo/



# DPO

DPO starts with preference data pairs(prompt, accepted and rejected) and a SFT model without the requirement of reward model. One thing worth mentioning is the preference data should be in distribution, i.e., it should be generated by the SFT model. Otherwise, consider fine-tuning the SFT model with some of the preference data to alleviate distribution shift before the actual DPO training,

# Reference

- [DPO (PPO代替)解读与实践【大模型论文系列】](https://zhuanlan.zhihu.com/p/655421669)
- [Is DPO Always the Better Choice for Preference Tuning LLMs?](https://deci.ai/blog/dpo-preference-tuning-llms/)
- [The N Implementation Details of RLHF with PPO](https://d2jud02ci9yv69.cloudfront.net/2024-05-07-the-n-implementation-details-of-rlhf-with-ppo-130/blog/the-n-implementation-details-of-rlhf-with-ppo/)