---
date created: Saturday, February 24th 2024, 10:26:40
date modified: Monday, March 4th 2024, 23:39:04
draft: "true"
tags:
  - transformers
---

# Attention mechanism


# Multi head attention

> [!question] 多头注意力机制和多层注意力层之间的区别
> 
> 多头： 多个注意头使得模型可以关注不同的子空间，从而提取不同的特征表示，增加模型对不同特征的敏感。
> 多层：实现更深层次的特征提取和组合，增加模型的深度和非线性能力，捕捉更高级别的特征和语义关系，提高模型在复杂任务上的性能。

# Self attention



# Variants of attention


## [Linear Attention](https://arxiv.org/pdf/2006.16236.pdf)

